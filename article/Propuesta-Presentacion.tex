\documentclass{beamer}

\mode<presentation>
{
  %\usetheme{Warsaw}
  % or ...
  \useinnertheme[hoptionsi]{rounded}
  \useoutertheme{split}
  \useoutertheme{shadow}
  \usecolortheme{seahorse}
  \setbeamercovered{transparent}
  % or whatever (possibly just delete it

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 

%\beamerdefaultoverlayspecification{<+->}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{Previous Work}

\begin{frame}
  \frametitle{Previous Work}
  \framesubtitle{Getting the most out of data}

  \begin{itemize}
  \item<1-> Better aid in 'Who wants to be a millionaire?': the audience. 
  \begin{itemize}
     \item We improve our confidence asking several 'experts'.
  \end{itemize}
  \item<2-> Ask several doctors before undergoing a major surgery.
  \begin{itemize}
    \item We weight and recombine the various opinions to get an answer.
  \end{itemize}
  \item<3-> Why Ensemble Based Systems and Arcing
    \begin{itemize}
    \item<4-> Bias vs. Variance.
    \item<5-> Amount of Data.
    \item<6-> Divide and Conquer.
    \end{itemize}
  \item<7->  Boosting: Incrementally combine moderately inaccurate rules to get an accurate rule.
\end{itemize}
\end{frame}

\section{Boosting}

\subsection{Training}
\begin{frame}
  \frametitle{Boosting (Schapire 1989)}
  Suppose we have a training set $S$ of size $N$ with labels $\omega_i \in \Omega = {\omega_1,\omega_2}$ and a weak learning algorithm {\bfseries WeakLearn}.
\\{\bfseries Training:}
  \begin{enumerate}
  \item<1-> Select randomly $N_1<N$ samples from $S$ to create $S_1$.
  \item<2-> Call {\bfseries WeakLearn} and train it with $S_1$ to create classifier $C_1$.
  \item<3-> Create the set $S_2$ as the most informative dataset, given $C_1$. Half correctly classified by $C_1$ and the other half misclassified.
%    \begin{itemize}
%    \item<4-> Flip a fair coin.
%    \item<4-> Tail: Add first instance misclassified by $C_1$ to $S_2$.
%    \item<4-> Head: Add first instance correctly classified by $C_1$ to $S_2$.
%    \item<4-> Continue until no more patterns can be added to $S_2$
%    \end{itemize}
  \item<4-> Train the classifier $C_2$ with $S_2$
  \item<5-> Create the set $S_3$ by selecting those instances for which $C_1$ and $C_2$ disagree. 
  \item<6-> Train classifier $C_3$ with $S_3$
  \end{enumerate}
\end{frame}

\subsection{Testing and Analysis}
\begin{frame}
  \frametitle{Boosting (Schapire, 1989)}
\begin{block}{Testing}
  \begin{enumerate}
  \item<1-> Classify instance $\mathbf x$ by $C_1$ and $C_2$.
  \item<2-> If they agree on the class, classify instance in that class.
  \item<3-> If they disagree ask $C_3$ where to classify the instance. That will be the final classification.
  \end{enumerate}
\end{block}
\begin{block}{Analysis}
  \begin{itemize}
  \item<4-> Ensemble error is bounded above, and is less than the error of the best classifier in the ensemble.
  \item<4-> Stronger classifiers can be obtained by recursive calling of the algorithm.
  \end{itemize}
\end{block}
\end{frame}

\section{AdaBoost}
\subsection{Introduction}
\begin{frame}
  \frametitle{AdaBoost (Freund \& Schapire, 1996)}
Sample subsets using a distribution that is modified in a way that the weight  of "hard" instances is increased, according to the last trained learner error.
\pause 
\\Final classifier is a linear combination of the weak classifiers iteratively trained.
\pause
\begin{itemize}
\item<3-> The element $D_t(i)$ in the distribution represents the weight of the instance $\mathbf x_i$ on iteration $t$.
\item<4-> The weak classifier trained on iteration $t$ is called the hypothesis $h_t$.
\item<5-> Hypothesis $h_t$ has an error $\epsilon_t$. The normalized error is called $\beta_t$
\end{itemize}
\end{frame}

\subsection{Algorithm Conceptualization}
\begin{frame}
  \frametitle{AdaBoost (Freund \& Schapire, 1996)}
\begin{block}{Procedure}
\begin{enumerate}
  \item<2-> Initialize $D_t$ with a uniform distribution.
  \item<3-> Build a hypothesis $h_t$ by calling {\bfseries WeakLearn} with training set according to distribution $D_t$.
  \item<4-> Calculate the error $\epsilon_t$ of $h_t$ and modify distribution $D_t$ to obtain $D_{t+1}$ in such a way that misclassified instanced are 'boosted'.
  \item<5-> Repeat from step 2 until $t \geq T$.
  \item<6-> Output classifier is a linear combination of hypotheses $h_t$.
\end{enumerate}
\end{block}
\end{frame}

\subsection{Binary Classification Algorithm}
\begin{frame}
  \frametitle{AdaBoost (Freund \& Schapire, 1996)}
\begin{block}{Training}
Given a sequence of labeled examples $\{({\mathbf x_1},y_1),...,({\mathbf x_N},y_N)\}$ with $y_i \in \{0,1\}$.
Initialize $D_t(i)=1/N$.
Do for t = 1, 2, . . . , T:
\begin{enumerate}[<+->]
\item Select a training data set $S_t$ using distribution $D_t$.
\item Train {\bfseries WeakLearn} with $S_t$ to obtain $h_t$.
\item Calculate $\epsilon_t=\sum_i D_t(i)|h_t({\mathbf x_i})-y_i|$.
\item Set $\beta_t = e_t/(1 - e_t)$.
\item Set $D_{t+1}(i)=\frac{D_t(i)\beta_t^{1-|h_t({\mathbf x_i})-y_i|}}{Z_t}$
%D_{t+1}(i)&=D_{t+1}(i)/\sum_i D_{t+1}(i)
\end{enumerate} 
Output $
h_f({\mathbf x_i})=\begin{cases} 
  1,  &\mbox {if }\sum_t (\log\frac{1}{\beta_t}) h_t({\mathbf x_i})\geq \frac{1}{2}\sum \log\frac{1}{\beta_t}\\
  0, & \mbox{otherwise}
\end{cases}
$
\end{block}
\end{frame}

\subsection{Training Error}
\begin{frame}
\frametitle{Training Error}
\framesubtitle{Is it Boosting?}
\begin{theorem}
The error $\epsilon=Pr_{i\sim D}[h_f({\mathbf x_i})\neq y_i]$ of the final AdaBoost hypothesis is bounded above by:
\begin{equation*}
\begin{split}
\epsilon\leq 2^T\prod_{t=1}^T \sqrt{\epsilon_t(1-\epsilon_t)}\\
= \prod_{t=1}^T \sqrt{1-4\gamma_t^2} \leq \exp\ \left(-2\sum_t \gamma_t^2 \right)
\end{split}
\end{equation*}
where $\gamma_t=1/2-\epsilon_t$
\end{theorem}
\end{frame}

\begin{frame}
\frametitle{Training Error}
\framesubtitle{cont.}
\begin{block}{Proof}
Given $w_i^{t+1}=w_i^{t}\beta_t^{1-|h_t({\mathbf x_i})-y_i|}$ :
\begin{center}
$\sum_{i=1}^N {w_{i}^{t+1}} \leq \left(\sum_{i=1}^N
w_i^t\right)(1-(1-\epsilon_t)(1-\beta_t)).$\\
$\sum_{i=1}^N {w_i^{T+1}} \leq \prod_{t=1}^T (1-(1-\epsilon_t)(1-\beta_t)).$
\end{center}
The final hypothesis makes a mistake if
\begin{center}
$\prod_{t=1}^T \beta_t^{-|h_t({\mathbf x_i})-y_i|} \geq \left(\prod_{t=1}^T \beta_t\right)^{-1/2}.$
\end{center}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Training Error}
\framesubtitle{cont.}
\begin{block}{Proof (Cont)}
The final weight of any instance is
\begin{center}
$w_i^t = D(i) \prod_{t=1}^T \beta_t^{1-|h_t({\mathbf x_i})-y_i|}.$
\end{center}
Using the two previous equations:
\begin{center}
$\sum_{i=1}^N w_i^{T+1} \geq  \sum_{i|h_f({\mathbf x_i})\neq y_i} w_i^{T+1} \geq \left( \sum_{i|h_f({\mathbf x_i})\neq y_i} D(i) \right)\left(\prod_{t=1}^T \beta_t\right)^{1/2}$
\\$\sum_{i=1}^N w_i^{T+1} \geq \epsilon\left(\prod_{t=1}^T \beta_t\right)^{1/2}$
\end{center}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Training Error}
\framesubtitle{cont.}
\begin{proof}[Proof (Cont)]
Using second and last equations:
\begin{center}
$\epsilon \leq \prod_{t=1}^T \frac{1-(1-\epsilon_t)(1-\beta_t)}{\sqrt{\beta_t}}$
\end{center}
Setting $\beta_t=\epsilon_t/(1-\epsilon_t)$ we get
\begin{center}
$\epsilon\leq 2^T\prod_{t=1}^T \sqrt{\epsilon_t(1-\epsilon_t)}$
\end{center}
\end{proof}
\end{frame}

\begin{frame}
\frametitle{Training Error}
\framesubtitle{cont.}
\begin{block}{Analysis}
This way AdaBoost is a procedure for finding a linear combination $f$ of weak hypotheses which attempts to minimize 
\begin{equation*}
\sum_i \exp(-y_i f({\mathbf x_i)})=\sum_i \exp\left(-y_i \sum_t \alpha_t h_t({\mathbf x_i)}\right).
\end{equation*}
\end{block}
\end{frame}

\subsection{Generalization Error}
\begin{frame}
\frametitle{Generalization Error}
Given $m$ the size of the sample, $d$ the VC-dimension of the weak hypothesis space and $T$ the number of rounds in boosting algorithm, Freund and Schapire showed that the generalization error, with high probability, is at most:
\begin{equation*}
\hat{P}r[h_f(x)\neq y] + O\left( \sqrt{\frac{Td}{m}} \right)
\end{equation*}
Which suggest over-fitting if run for too many rounds.\\
\alert{Early experiments shown that boosting does not overfit, even when run for thousands of rounds.}
\end{frame}

\begin{frame}
\frametitle{Generalization Error}
\framesubtitle{Another Aproach}
We define the margin of example $({\mathbf x},y)$ as
\begin{equation*}
\mbox{margin}_f({\mathbf x},y)=\frac{y\sum_t \alpha_t h_t({\mathbf x})}{\sum_t |\alpha_t|}
\end{equation*}
It is a number in [-1, 1] which can be interpreted as the confidence of prediction.
\\This way we get the bound:
\begin{equation*}
\hat{P}r[\mbox{margin}_f({\mathbf x},y)\leq\theta] + O\left( \sqrt{\frac{d}{m\theta^2}} \right)
\end{equation*}
\end{frame}

%\begin{frame}
%\frametitle{Generalization Error}
%\framesubtitle{Example}
%\begin{figure}
%\centering
%\includegraphics[scale=0.8]{traintest.pdf}
%\caption{Descripción mecánica y análisis de los dedos humanos. Algunas articulaciones como la MCP (metacarpo-falángica) tienen más de un grado de libertad, como flexión-extensión y aducción-abducción.}
%\label{fig:1}
%\end{figure}
%\end{frame}

\subsection{Multiclass Algorithm}
\begin{frame}
  \frametitle{AdaBoost (Freund \& Schapire, 1996)}
\begin{block}{Training}
$y_i \in \{1,...,k\}$. Initialize $D_t(i)=1/N$.
Do for t = 1,...,T :
\begin{enumerate}[<+->]
\item Select a training data set $S_t$ using distribution $D_t$.
\item Train {\bfseries WeakLearn} with $S_t$ to obtain $h_t$.
\item Calculate $\epsilon_t=\sum_{i}D_t(i)[[h_t({\mathbf x_i})\neq y_i]]$. If $\epsilon_t \geq 1/2$ abort loop.
\item Set $\beta_t = e_t/(1 - e_t)$.
\item Set $D_{t+1}(i)=\frac{D_t(i)\beta_t^{1-[[h_t({\mathbf x_i})\neq y_i]]}}{Z_t}$
%D_{t+1}(i)&=D_{t+1}(i)/\sum_i D_{t+1}(i)
\end{enumerate} 
Output $
h_f({\mathbf x_i})=\arg\max_{y \in Y}{\sum_t\left(\log(\frac{1}{\beta_t}\right)[[h_t({\mathbf x})\neq y]]}
$
\end{block}
\end{frame}

%\begin{frame}
%\frametitle{AdaBoost.M1 (Freund \& Schapire, 1997)}
%\begin{block}{Testing}
%\begin{enumerate}[<+->]
%\item Obtain total vote received by each class:
%\begin{equation*}
%V_{j}=\sum_{t|h_t({\mathbf x})= C_j} \log \frac{1}{\beta_t}
%\end{equation*} 
%\item Select class with highest total vote.
%\end{enumerate}
%\end{block}
%\end{frame}

\section{Summary*}

\begin{frame}
  \frametitle<presentation>{Summary}

  % Keep the summary *very short*.
  \begin{itemize}
  \item
    Boosting is a technique which gives a strong hypothesis from a set of relatively weak hypotheses.
  \item
    AdaBoost decreases its training error asymptotically given that the hypotheses error is better than random guessing.
  \item
    AdaBoost performs well with generalization error due to its minimization of instances margin. Generalization error is not related to the number of rounds.
  \end{itemize}
  
  % The following outlook is optional.
  %\vskip0pt plus.5fill
  %\begin{itemize}
  %\item
  %  Outlook
  %  \begin{itemize}
  %  \item
  %    Something you haven't solved.
  %  \item
  %    Something else you haven't solved.
  %  \end{itemize}
  %\end{itemize}
\end{frame}


\end{document}


