% \chapter{A topology for multi-goal Neural Fields} \label{ch:chp3}
\section{Introduction}
\label{sec:chp3-introduction}
In previous chapters it has been shown how a neural field can be used
as a controller for a unstable system. Neural fields have performed
good enough (compared to the RNN approach) when applied to a simple
control problem, a cart-and-pole (inverted pendulum) system. The
particular configuration tested was composed by:

\begin{itemize}
\item Two input variables: angular position and angular speed.
\item One control goal: to minimize the angular position.
\item One control action: lateral force at the pendulum base.
\end{itemize}

The corresponding controller architecture was devised as a two layer
neural field. Each so-called layer of the field is actually a neural
population modeled after a subset of the real line: $x | x \in
(-1,1)$. 
Those layers have the following roles:

\begin{itemize}
\item A first layer, without inner dynamics (i.e. not governed by a
  differential equation) which merely maps the input variables to
  activation potentials on a field, using the vector coding technique.
\item A second layer, with prototypical dynamics of neural fields. The
  kernel function was used as the tuning/evolution parameter from
  which the control behavior emerged. The actual output was obtained
  by finding a weighted average of the activation potentials and
  mapping the resulting centroid to a single value (an inverse process
  to vector coding).
\end{itemize}

While this approach yields favorable results, there are some aspects
worth improving. The first one is the rather naive topological
structure of the populations, as they are modeled as close as possible
to the control action space, but present singularities at the extreme
points of the real line segment. Those singularities are not present
in the model developed by Amari, and can be expected to alter the
behavior of the population as the points drift away from the origin.

The second one is the parameterization scheme applied. The variation
of the kernel function, represented as a value list, preclude the
analysis of the inner workings of the control architecture. As the
resulting function gives no guaranties about its structure (not even
continuity or differentiability). Also they somehow water down the
spatial structure: there is no standard interpretation about the
relationship between close or distant elements in the population.

A third aspect is the scalability of the architecture. Having only an
input layer and an action layer, the control architecture has no room
for separation between the control goals and the control actions. A
first consequence of that is the lack of flexibility to have a scheme
with two control goals and two control actions (where each control
goal is supossed to act on each one of the control actions).

In what remains of this chapter, we present changes to the
control architecture and the topology of the population specifically
aimed to address these limitations. We also use the proposed
architecture to control the inverted pendulum with two goals:
stability and linear position.

\section{Topological Structures for Multiple Goals}
\label{sec:chp3-topological}
\subsection{Topology of Neural Populations}
As said previously, each neural population was previously modeled
after a segment of the real line. The obtained topology has two
extremal point not so well behaved. Particularly, those elements
laying at the extremal point will only have neighbors at one
side. Also, the number of neighboring elements located on each side
will be different, with exception to the middle element. Therefore,
the translational symmetry present in the model of Amary is lost, and
instead there is only a reflexive symmetry at the middle element.

It would be convenient to use a space that is closed and bounded, that
viewed as a manifold does not have boundary, and that has a continuous
symmetry group. This can be attained by applying the one-point
Alexandroff compactification of the Real line, by including a $\infty$
element. Therefore, the neural population will be modeled after a
unitary circle embedded in $\mathbb{R}^2$. It is defined as the
Co-domain of a map $f: (-\pi,\pi]\rightarrow \mathbb{R}^2$, $f:\theta
\mapsto r=(x_1,x_2)$ where:
\begin{align}
  x_1 &= cos(\theta)\\
  x_2 &= sin(\theta)
\end{align}

Consequentky, the emerging symmetry group is the orthogonal group
O(2), composed by the rotations of the circle SO(2) and its
reflections.

In order to evaluate the kernel function, it is needed a metric over
the above defined space. Given the Euclidean metric on $\mathbb{R}^2$,
the distance between two points located on the circle, parameterized
by $\theta_1$ y $\theta_2$, will be evaluated as the lower valued path
integral over the circle (the geodesic).
\begin{equation}
  \int_{\theta_1}^{\theta_2}\sqrt{\left(\frac{dx_1}{d\theta}\right)^2
    + \left(\frac{dx_2}{d\theta}\right)^2}d\theta
\end{equation}

Nonetheless, we also intend to modify the parameterization scheme used
previously, where the kernel function was used directly as the set of
parameters. Instead, we will provide each population with its own
metric. Consequently, using a positive definite symmetric matrix, a
metric tensor $g$, the actual expression used to evaluate the distance
is:
\begin{equation}
  s(\theta_1,\theta_2)=\min\left(\int_{\theta_1}^{\theta_2}\sqrt{\dot{r}(\theta)^TG\dot{r}(\theta)}d\theta,\int_{\theta_2}^{\theta_1}\sqrt{\dot{r}(\theta)^TG\dot{r}(\theta)}d\theta\right)
\end{equation}

One remaining issue to be solved is how to evaluate the distance
between two elements in different populations. As in general two
populations are not path connected, the above presented notion of
metric does not apply. That distance is needed when an element of a
population $a \in A$ receives as input the potential of an element $b
\in B$, or using another terminology, when $b$ is in the receptive
field of $a$. Therefore, to evaluate the distance, a simple rule can
be applied.

In general, the distances between an element $a$ and another element
$b$ will be evaluated using the metric of the receiving element (the
owner of the receptive field). In order to allow that, each element
$b$ in a population $B$ will be mapped to a location in the underlying
space $Y$ of that population. As every population is modeled after a
S1 space (differing only in the metric tensor), to map the element, it
will be used the $O(2$ symmetry group.

More explicitly, for an element with position in the underlying space
$y \in Y$, and another element with position in another underlying
space $x \in X$ in the receptive field of the element at $y$, the
distance $d(y,x)$ will be evaluated using a function $T_{X,Y}: X
\rightarrow Y$, $T_{X,Y}:\theta \mapsto T_{X,Y}(theta)$ where the
actual mapping will be given by applying an element of the circle
symmetry group O(2). The element of O(2) will be the same for the
entire mapping (in the simplest case, the identity element can be
applied, so that the element will keep its $\theta$ parameterization,
but will be assumed to lay in the target space). With this definition
we obtain a distance $d(x,y)=s(\theta_x,T_{Y,X}(\theta_y))$, that is
applicable even when $x,y \in Y$, where it will be used the identity
transformation $T_{Y,Y}=1_Y$. It is worth noting, that in the general
case $d(x,y)$ cannot be considered as a complete metric over the union
of several spaces because it is not symmetric (i.e. $d(x,y) \neq
d(y,x)$), because $d(y,x)$ may even be not defined (when $x$ is not in
the receptive field of $y$).


\subsection{Neural Field Controller Architecture}
In this chapter, the neural field controller architecture that will be
used has three layers: one input layer, one processing layer and one
output layer. Each layer has several neural populations (each one with
its own underlying space and metric). A neural population in one layer
cannot receive inputs from other populations except those in the
immediately preceding layer (with \emph{output} receiving from
\emph{processing}, and \emph{processing} receiving from
\emph{input}. The layer are further described below:

\paragraph{Input Layer}
Each population on this layer is used as a spacial representation of a
function of the inputs. While this function can be general, here it
will be simplified. As a result, it is used one input population for
each relevant state of the controlled system (where relevant means
related to the control goal). It should be noted that this layer is
not ruled by a differential equation (also do not use a metric or
kernel) and รง therefore are not related with Amari's definition of a
neural field.

The function used depends on the space where the state variable is
defined. For an angular value $\theta_{in}$, it assigns an unitary
potential to the neural population element located (or parameterized
by) $\theta_{in}$. For a linear value $x_{in}$, it assign also a
unitary potential to the element located in the reverse stereographic
projection (as shown in the figure) $\theta=2\arctan
(x_{in})$.  %TODO: include figure

\paragraph{Processing Layer}
Neural populations on this layer behave according to a differential
equation where the inputs are the potentials of other elements of the
population, as well of those elements in other populations of the
previous layer that are in the receptive field of the element (for
simplicity it will be assumed that every population in the preceding
layer is in the receptive field of each element in a given
population. Therefore, those populations in this layer are parallel to
the definition given by Amari for neural fields (they differ in the
use of metric tensors, in the topology of the underlying space and in
the mapping used to connect elements in one population to elements in
another one).

The differential equation for the potential of an element at $\theta$
in a population $Y$ of the processing layer is:

\begin{align}
  \label{eq:lnf-oned}
  \tau_Y\dot{u}_Y(\theta)&=-u_Y(\theta)+\int_{-\pi/2}^{\pi/2}{w_{Y,Y}\left(s_Y(\theta,\theta')
    \right) f\left(u_Y(\theta') \right) d\theta'} + S_Y(\theta) \\
  S_Y(\theta)&=\sum_{X \in
    \mathrm{In}}\int_{-\pi/2}^{\pi/2}{w_{X,Y}\left(s_Y(\theta,T_{X,Y}(\theta'))
    \right) f\left(u_X(\theta') \right) d\theta'}
\end{align}
Where as in previous chapters $\tau_Y$ is a time constant for the
population $Y$, $u_Y$ is the activation potential of elements in the
population, $w_{X,Y}$ is the connection kernel for connections from
population $X$ towards population $Y$, $s_Y$ is distance (or geodesic
length) between two positions on the underlying space of population
$Y$ using the given metric tensor for that space, $X \in $ In is a
population in the Input Layer, and $T_{X,Y}$ is a transformation of
the orthogonal symmetry group of the circle O(2) fixed for the ordered
pair of populations.

\paragraph{Output Layer}
%Transformation (symmetry group) and summation

%Centroid

%Stereographic Projection

\subsection{Parameterization of Neural Fields}


\section{Co-evolution and Complexification}
\label{sec:chp3-coevolution}

\section{Acquisition of Objectives by Evolution}
\label{sec:chp3-acquisition}

\section{Experimental Framework}
\label{sec:chp3-experimental}

\section{Experimental Results}
\label{sec:chp3-results}

\section{Discussion}
\label{sec:chp3-discussion}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
