% \chapter{On information processing in neural field controllers} \label{ch:chp3}
\section{Introduction}
\label{sec:chp3-introduction}
This chapter introduces the Simplest Biped Walking problem, which will
be used onwards as the reference control problem. Approximations to
optimal linear state-feedback controllers for the Simplest Biped
Walking problem are found using numerical methods (genetic
algorithms), for each sub-region on a delimited state-space region. A
general architecture for the neural field controllers is presented,
and an structured strategy to implement non-linear (sliding-mode)
state-space controllers using the neural field-like controller
architecture is shown. Finally, an specific controller for biped
walking is developed and tested using the collected data, its behavior
is shown using a Poincaré section for the controlled biped dynamics,
and an stability analysis is provided.

\section{The Simplest Biped Walking Model}
\label{}

The Simplest Walking Model, presented by Garcia et
al. \cite{Garcia98Simplest}, is an attempt to create the simplest
model that is capable of mimicking bipedal gait. It provides a
reference model to study the phenomena that allows walking in two
dimensions, and it has been further studied in terms of stability
\cite{Schwab01Basin} \cite{Das02Alternative}, swing-leg control
\cite{Wisse05How}, energy consumption in active walking
\cite{Kuo02Energetics}, walking on stairs \cite{Tehrani07Stairs}, and
walking with an upper body \cite{Wisse04Passive}.

Here is shown the model of Garcia et al., modified to allow control
torques \ref{fig:chp3-simplest}.

The 2D Biped Walking (yet to be simplified) model assumes a biped with
a hip connected to two feet through rigid legs with no knees. It has a
point mass $M$ located at the hip, and two point masses $m$ located at
the feet, with ratio $\beta = m/M$. This model has a swing phase,
where there is a leg in contact with the ground (stance leg), and
another one in pendular motion (swing leg). The collision of the
swinging foot with the ground at heel-strike is plastic (no-slip,
no-bounce), and causes its velocity to jump to zero. Also, the double
support is instantaneous, so only one leg is in contact with the
ground at any time. For simplicity, it is assumed that the swing leg
is allowed to pass through the floor surface an be below floor level
once on each step, and only its second crossing will be detected as a
collision (otherwise the foot-scuffing problems inevitable for a
walker with straight legs would appear).

The equations of motion for the swing phase have the form $T =
H(q)\ddot{q}+C(q,\dot{q})\dot{q} + \tau_g(q)$, with
$q=[\theta\;\phi]^T$, where:

\begin{align*}
  H &= \left[ \begin{array}{cc}
      1+2\beta(1-\cos\phi) & -\beta(1-\cos\phi) \\
      \beta(1-\cos\phi) & -\beta \end{array} \right] \\
  C &= \left[ \begin{array}{cc}
      2\beta\dot{\phi}\sin\phi & -\beta\dot{\phi}\sin\phi \\
      \beta\dot{\theta}\sin\phi & 0 \end{array} \right] \\
  \tau_g &= \left[ \begin{array}{c}
      (\beta g/l)[\sin(\theta-\phi-\gamma)-\sin(\theta-\gamma)]-(g/l)\sin(\theta-\gamma) \\
      (\beta g/l)\sin(\theta-\phi-\gamma) \end{array} \right] \\
  T &= \left[ \begin{array}{c}
      T_{\theta} \\
      T_{\phi} \end{array} \right]
\end{align*}

The Simplest Biped Walking model is achieved simplifying the previous
model, by assuming that the point masses located at the feet are
infinitesimal in comparison to the point mass at the hip (i.e. when
$\beta \rightarrow 0$). In the first row $\beta$ is set to $0$, and in
the second row we divide by $\beta$, effectively isolating the stance
leg acceleration $\ddot{\theta}$ from the swing leg angle
$\phi$. Therefore:

\begin{align}
  H &= \left[ \begin{array}{cc}
      1 & 0 \\
      1-\cos\phi & -1 \end{array} \right] \\
  C &= \left[ \begin{array}{cc}
      0 & 0 \\
      \dot{\theta}\sin\phi & 0 \end{array} \right] \\
  \tau_g &= \left[ \begin{array}{c}
      -(g/l)\sin(\theta-\gamma) \\
      (g/l)\sin(\theta-\phi-\gamma) \end{array} \right]
\end{align}

Furthermore, we now define the action torque vector as:

\begin{equation}
  T = \left[ \begin{array}{c}
      T_{\theta} \\
      T'_{\phi} \end{array} \right]
\end{equation}

Where we scale the torque on $\phi$: $\beta T'_{\phi} =
T_{\phi}$. Also, it is required a transition mapping for the state
vector before and after collision. As shown by Garcia et al., there is
a phase reduction in the collision, from four dimensions to two
dimensions, which is seen as a rank reduction in the mapping equation:

\begin{equation}
  \left[ \begin{array}{c}
      \theta \\
      \phi \\
      \dot{\theta} \\
      \dot{\phi} \end{array} \right]^+
  = \left[ \begin{array}{cccc}
      -1 & 0 & 0 & 0 \\
      -2 & 0 & 0 & 0 \\
      0 & \cos 2\theta & 0 & 0 \\
      0 & (1-\cos 2\theta)\cos 2\theta & 0 & 0 \end{array} \right]
  \left[ \begin{array}{c}
      \theta \\
      \phi \\
      \dot{\theta} \\
      \dot{\phi} \end{array} \right]^-
\end{equation}

To complete the model, it is introduced the heel-strike event
condition (also from Garcia et. al), that provides a geometric
condition in terms of the state vector, that has to be satisfied for
the swing foot to cross the ramp surface:

\begin{equation}
  \phi^+ - 2\theta^- = 0
\end{equation}

Given that there is a model dimension reduction on each heel-strike,
each step is uniquely determined by the initial state after
heel-strike \( v^+=\big[\begin{smallmatrix}\theta^+ \\
  \dot{\theta^+} \end{smallmatrix} \big]\). The stability of a given
controller can be analyzed through the evolution of the Poincaré
section $v^+_n$ after the $n$-th heel-strike. This is analogous to
analyzing the stability of the 'stride function' $v_{n+1}=S(v_n)$ in
terms of McGeer \cite{?}. In general, stability in this chapter is
understood in the sense of Lyapunov, where the stride function is
taken as a discrete system. As there are some $v$ values for which the
stride function $S(v)$ has no output, it is assumed that stability is
not attained when, for a given $v_0$, there is a $n$ for which
$v_n=S^n(v_0)$ is not defined.

\section{Searching for Optimal Linear State-Feedback Controllers}
\label{sec:chp3-searching}

As shown by Schwab et al. \cite{Schwab01Basin}, while there is an
attractor for the Simplest Biped Walking model on small slopes, its
basin of attraction covers a minute portion of the initial
configurations on phase space. Wisse et al. propose a controller for
the swing leg that substantially widens the basin of attraction
\cite{Wisse05How}, that may be equally implemented as a spring-damper
mechanical configuration (aided by some switching mechanism when the
swing leg becomes the stance leg), or as a state-feedback
control. This controller is based on the intuition that a rimless
wheel attains asymptotically stable steady 2D motions, given the
proper ground slope and angle between legs
\cite{Coleman97Motions}. Also, it has the advantage that, for
sufficiently small mass ratio $\beta$, the energy cost of the required
control action is negligible. Such control strategy can be stated as:
\begin{equation} \label{eq:chp3-control-action}
  T'_{\phi}=k_{\phi}(\phi_r-\phi)+\sqrt{k_{\phi}}\dot{\phi}
\end{equation}
Where the reference swing leg angle is set to a convenient value
$\phi_r=0.3$.

The aforementioned control strategy provides a wider basin of
attraction as $k_{\phi}$ grows. Therefore, for each initial
configuration, there is a minimum $k_{\phi}$ that attains stable
walking \cite{Wisse05How}.

As a first step to extending the static control policy of Wisse et
al., in this section there will be found a set of minimal $k_{phi}$
values that attain stability for each initial configuration, with a
given discretrization of the configuration space.

For the purpose of comparison, the system with $\gamma=0.004$, and $M
= g = l = 1$ will be explored. The initial configuration space will
have the bounds $\theta_0 = [0, 0.4]$ and $\dot{\theta}_0 = [-0.4,
0]$, with step size $\delta v=0.025$, for a 17x17 nodes grid. For each
node in the grid, a search with an evolutionary algorithm is
performed, using $k_{\phi}$ as the genotype. The fitness function
evaluation implies running a simulation of the model with the
state-feedback controller using the $k_{\phi}$ given, for a fixed time
interval. The simulation may abort before the fixed time, if state
variable (not the Poincaré section) $\theta(t)$ leaves the interval
$[-\pi/2, \pi/2]$, and the lowest fitness value is
assigned. Otherwise, the fitness value is $k_{\phi}^2$.

\begin{figure}[ht]
  \label{fig:figure1}
  \centering
  \includegraphics[scale=0.9]{K_values_17x17}
  \caption{Search result for the mapping
    $k_{\phi}=M(\theta_0,\dot{\theta_0})$.}
\end{figure}


The resulting mapping $k_{\phi}=M(\theta_0,\dot{\theta_0})$ is shown
in the figure \ref{fig:chp3-kvalues} for $\gamma=0.004$. As can be
seen, the mapping is monotonically increasing both as $\theta_0$
increases and as $\dot{\theta}_0$ decreases.

Before applying this mapping to an extended control policy for the
Simplest Biped Walking model implemented with a neural fields-like
structure, an architecture for neural field controllers is required.

\section{A Neural Field Controller Architecture}
\label{sec:chp3-neural}

In previous chapters it has been shown how a neural field can be used
as a controller for an unstable system. Neural fields have performed
good enough (compared to the RNN approach) when applied to a simple
control problem: a cart-and-pole (inverted pendulum) system. The
particular configuration tested was composed by:

\begin{itemize}
\item Two input variables: angular position and angular speed.
\item One control goal: to minimize the angular position.
\item One control action: lateral force at the pendulum base.
\end{itemize}

The corresponding controller architecture was devised as a two layer
neural field. Each layer of the field is actually a neural population
modeled after a subset of the real line: $x | x \in (-1,1)$.  Those
layers have the following roles:

\begin{itemize}
\item A first layer, without inner dynamics (i.e. not governed by a
  differential equation) which merely maps the input variables to
  activation potentials on a field, using the vector coding technique.
\item A second layer, with prototypical dynamics of neural fields. The
  kernel functions (for connections between layers 1 and 2, and for
  recurrent connections on layer 2) were used as the tuning/evolution
  parameters from which the control behavior emerged. The actual
  output was obtained by finding a weighted average of the activation
  potentials and mapping the resulting centroid to a single value (an
  inverse process to vector coding).
\end{itemize}

To generalize this approach, here is introduced a simple architecture
of neural field controllers to help the understanding of commonalities
and variations of the several controller structures developed next. It
is not meant as a global framework for control with neural fields, as
other architectures may be fruitfully developed. It is instead, an
specific model for the description of controllers based on layered
neural fields, able to describe the controller developed in this
chapter. The controller architecture that will be used here onwards is
described with an emphasis in how information is processed in the
controller.

The neural field controller architecture will be generally structured
as follows. It will have three layers:
\begin{itemize}
\item[Input:] Vector-coded represention of relevant input values
  (e.g. a subset or linear combination of controlled system
  state-variables).
\item[Representation:] Inner state representation of the controller
  architecture. Could be interpreted as the probability of being on
  each state $p(s=s_x)$ from the viewpoint of the controller.
\item[Action:] Representation of the preferred actions of the
  controller. May be interpreted as vector-coded (when using a
  complete neuron population), or directly coded (when using a single
  node for each controlled variable).
\end{itemize}
 
As a strongly layered architecture, the connections are only allowed
from contiguous layers, in one direction. Thus, elements in one layer
cannot receive inputs from other elements except those in the
immediately preceding layer (with \emph{action layer} receiving from
\emph{representation layer}, and \emph{representation layer} receiving
from \emph{input layer}).

Each of these layers may or may not have a dynamic behaviour, and
furthermore, if it does, may have the form of a neural field, in terms
of structure and behavior (i.e. governed by the dynamic equations of
neural fields).

For those layers that do have a dynamic behavior (as is the case of
the representation layer in this chapter, as will be shown next), the
derivative of the layer potential at each position $\dot{u}_i(x)$ is
fed with the output of function from the previous layer activation
potential $f_{i,i-1}(u_{i-1})$ (and may have also a recurrent
component, function of $u_i$).

Those layers that do not have a dynamic behavior, have their input
directly defined as a function of the previous layer activation
potential $u_i=f_{i,i-1}(u_{i-1})$. They also can be thought of as a
reference-tracking dynamical system with zero reference position
error, which is fast enough for its transient behavior to be
ignored. The input layer may be a particular case of this, where there
is no previous layer, and its activation potential is a function of
the controlled system state-variables $u_1=f_1(X)$.


\section{An Sliding-mode Neural Field Controller}
\label{sec:chp3-sliding}

\subsection{Controller Structure}

\begin{figure}[ht]
  \label{fig:chp3-cont-arch}
  \centering
  \includegraphics[scale=0.55]{controller_arch_chp4}
  \caption{Neural field controller architecture for Chapter 3.}
\end{figure}

The layers, depicted in figure \ref{fig:chp3-cont-arch} are further
described below:

\paragraph{Input Layer}
This layer is used as a spacial representation of a function of the
inputs, without a dynamic behavior (it does not add state-variables to
the bundled system-controller differential equation). The values
stored in this layer act only as a buffer to feed the inputs to the
representation layer, and correspond in each iteration to a mapping of
the system state-variables. It should be noted that this layer is not
governed by a differential equation and therefore does not follow
Amari's definition of a neural field.

While the mapping function can be general, here it will be
simplified. As a result, the input layer is a direct, vector-coded,
representation of the relevant state of the controlled system after
heel-strike. More specifically, for the input layer population
($i=1$):

\begin{align}
  u_1(x,t) &= f_I(x,v^+) \\
  f_1(x,v^+) &=
  \begin{cases}
    \text{const.} & \text{if v.c\,}(v_n^+) = x. \\
    0 & \text{otherwise.}
  \end{cases} \\
  x &= \big[\begin{smallmatrix} x_1 \\ x_2 \end{smallmatrix} \big] \in [0,1] \times [0,1] \\
  v_n^+ &= \big[\begin{smallmatrix}
    \theta_n^+ \\
    \dot{\theta_n}^+ \end{smallmatrix}\big] \in [0,0.4] \times
  [-0.4,0]
\end{align}

Note that the activation potential of the input layer $u_1(x,t)$ is a
function of the reduced system state-vector $v_n^+$ for the $n$-th
heel-strike (the last ocurred at time $t$), and is constant up to the
next heel-strike.

\paragraph{Representation Layer}
The neural population in this layer behaves according to a
differential equation where the input if functions of the activation
potential of the input layer. Therefore, the population in this layer
resembles the definition given by Amari for neural fields
\cite{Amari}, but with an specific structure for the interconnection
between input layer and representation layer populations, and also, a
minor role of recurrency.

The differential equation for the potential at position $x \in
\Omega_2=[0,1] \times [0,1]$ in the representation layer ($i=2$)
population is:

\begin{align}
  \label{eq:lnf-oned}
  \tau_2\dot{u}_2(x,t) &= -u_2(x,t) + \int_{x' \in
    \Omega_2}{w_{2,2}\left( s_{2,2}(x,x') \right)
    \psi \left(u_2(x',t) \right) dx'} + f_{2,1}(x,t) \\
  f_{2,1}(x,t)&=\int_{x' \in \Omega_1}{w_{2,1}\left(s_{2,1}(x,x')
    \right) \psi \left(u_1(x',t) \right) dx'}
\end{align}

Where as in previous chapters $\tau_i$ is a time constant for the
population $i$, $u_i$ is the activation potential of elements in the
population, $w_{j,i}$ is the connection kernel for connections from
population $i$ towards population $j$, which is a function of
$s_{j,i}$, the distance between two positions (on the same population
$j$, or in different populations $i$ and $j$), $Omega_i$ is the set of
positions $x$ in the layer $i$, and $\psi$ is the activation function
(here a Heaviside function).

The connection kernel $w_{j,i}$ could take any form, but the form of a
Mexican-hat function will be used.

\paragraph{Action Layer}
Similarly as it was done between the input layer, the action layer is
non-differential in nature (so it also does not add state variables to
the bundled system-controller differential equation). The mapping
between the representation layer and the action layer ($i=3$) is
evaluated using a weighting matrix, calculated as:

\begin{equation}
  \label{eq:eqn-outlayer}
  u_3(t) = W_{3,2}(x')u_2(x',t)
\end{equation}

Where $W_{3,2}(x')$ is the estimated control action if the current
state where located in the cell with center at position $x'$. The
actual values for $W_{3,2}$ are assigned to the resulting $K$ of the
search performed in section \ref{sec:chp3-searching}.

In the controller presented in this chapter, the action layer contains
only one node, and its activation corresponds to the controller
output, $k_{\phi}$, the proportional constant shown in section
\ref{sec:chp3-simplest}.

\subsection{Insights on the Controller Behavior}

The actual controller that is presented in this chapter may be
understood as a biologically implementation of a Sliding-mode
controller, and works (in general terms) selecting a control policy
for each step (providing as output the parameter $k_{\phi}$), using as
input the Poincaré section \( v_n^+=\big[\begin{smallmatrix}\theta_n^+ \\
  \dot{\theta_n^+} \end{smallmatrix} \big]\). The mapping
$k_{\phi}=M([\theta_n^+ \dot{\theta_n^+}]^T$ is embedded in the
structure of the neural field. An interpretation of the controller
structure is detailed next.

The input layer a two dimensional neural population, with one
dimension for $\theta_n^+$ and another for $\dot{\theta_n^+}$, the
elements in the Poincaré section $v_n^+$ (just after heel-strike), but
rescaled to be in the set $[0,1] \times [0,1]$. For these populations,
the function $f$ in the equation $u_1(x,t)=f(v_n^+)$ (shown in the
previous subsection) performs a vector coding, i.e. an specific value
of the input variable $v_n^+$ causes an activation $u_1(x)$ with
amplitude 1 at some position $x$, and an activation of 0 at any other
position. For implementation purposes, note that this value does not
need discretization, as the input layer is not explicitly modeled,
given that it does not add state variables to the bundled
system-controller differential equation.

The representation layer has one neural population, where the position
$x$ resides in the same two-dimensional space of the input layer
population. An activation of an element of the input layer population
(e.g the one corresponding to $v_n^+$) causes an activation bump that
reaches its maximum at some position in the representation layer, and
decreases as the row are farther away. This configuration gives the
representation population an structure that resembles the Poincaré
section space. The implementation has 17x17 elements, an equal number
than that of the mapping found in the section
\ref{sec:chp3-searching}, but it should be noted that the actual
points are different, given that in the representation and input
layers position grid, each node is located at the center of each cell
(and in the section \ref{sec:chp3-searching} were located at the cell
boundaries.

***

The output layer has also one population, actually with only one
element, which activation provides the output value $k_{\phi}$. Each
element in the processing population contributes with a value given by
the product of the activation and the optimal $k_{\phi}$ value for
that position in the Poincaré section. The total output is calculated
as the weighted average:

\begin{align}
  u_{out}(x,t)&=g(u_{proc}(x',t)) \\
  g(u_{proc}) &= \frac{u_{proc}(x,t)M([\hat{\theta_n}
    \hat{\dot{\theta}_n}]^T=x)}{\int_{x' \in
      \Omega_{proc}}u_{proc}(x',t)dx'}
\end{align}


\section{Experimental Results}
\label{sec:chp3-results}
The experimental results are shown in the figures. For comparison
purposes, the State-feedback (SF) control strategy proposed by Wisse
et al. is implemented, and put side to side to the Sliding-mode(-like)
Neural Field controller (SMNF), as proposed in this chapter. The
experiment shown is run for $t=[0, 100]$ with initial configuration
$v_n=[0.02 -0.39]^T$, which is near the most unstable point in the
region studied.

While the SF controller. keeps constant the $k_{\phi}=100$ value in
order to attain stability, the SMNF controller proposed recalculates
the $k_{\phi}$ on each step using the current Poincaré section and the
embedded mapping $k_{\phi}=M(v_n)$. As the output value of the neural
field uses a centroid policy, it interpolates between the values on
the 17x17 mapping grid. This interpolation could potentially cause the
loss of stability (because the linear interpolation could underestimate
the required $k_{\phi}$, therefore a displacement
$k_{\phi}=M(v_n+[0.5h -0.5h]^T)$ in the mapping is applied,
guaranteeing stability given the monotonically increasing nature of
the mapping (where $h$ is the mapping grid step).

\begin{figure}[ht]
  \label{fig:figure1}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.4]{SF_K}
    \caption{SF controller: $k_{\phi}$ vs time.}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.4]{SMNF2_K}
    \caption{SMNF controller: $k_{\phi}$ vs time.}
  \end{minipage}
\end{figure}

The overall performance of the SMNF controller is better than the SF
in terms of energy consumption and actuator strain, progressively
diminishing its $k_{\phi}$ value and thus the control action (see the
figure \ref{} where the K values vs time are plotted). The SF
controller converges faster to its fixed point (as would be expected
for a greater control action), but even in steady-state the control
action stays almost unaltered.

\begin{figure}[ht]
  \label{fig:figure1}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.4]{SF_PS}
    \caption{SF controller: Poincaré Section.}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.4]{SMNF2_PS}
    \caption{SMNF cont..: Poincaré Section.}
  \end{minipage}
\end{figure}

The Poincaré Section (see figure \ref{}) of the biped using the SMNF
controller shows how the system moves from a configuration that
requires higher $k_{\phi}$ values to configurations that require lower
ones, and that fact is exploited by the controller. The SF controller
also moves to configuration that require lower $k_{\phi}$ values (even
closer to the natural attraction basin of the biped), but that fact
remains unused.

\begin{figure}[ht]
  \label{fig:figure1}
  \centering
  \includegraphics[scale=0.4]{SF_T}
  \caption{SF controller: Time simulation.}
\end{figure}

\begin{figure}[ht]
  \label{fig:figure1}
  \centering
  \includegraphics[scale=0.4]{SMNF2_T}
  \caption{SMNF controller: Time simulation.}
\end{figure}

The time simulation (see figure \ref{}) shows how the SMNF changes the
control policy (as parametrized by $k_{\phi}$) softly enough to
provide a qualitatively natural gait for the biped. It should be noted
that sudden changes of behavior are common in Sliding-mode
controllers, but that is mitigated in this case by three facts: 1) The
neural field applies an interpolation using the centroid of activation
to calculate the output $k_{\phi}$. 2) The neural field has natural
dynamics qualitatively equal to a low-band filter. 3) The change in
the $k_{\phi}$ occur at a non-linear point in the biped dynamics that
anyway would cause a jump in its state.

\section{Discussion}
\label{sec:chp3-discussion}
**TBD**

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 

