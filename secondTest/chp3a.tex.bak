% \chapter{On information processing in neural field controllers} \label{ch:chp3}
\section{Introduction}
\label{sec:chp3-introduction}
In previous chapters it has been shown how a neural field can be used
as a controller for an unstable system. Neural fields have performed
good enough (compared to the RNN approach) when applied to a simple
control problem, a cart-and-pole (inverted pendulum) system. The
particular configuration tested was composed by:

\begin{itemize}
\item Two input variables: angular position and angular speed.
\item One control goal: to minimize the angular position.
\item One control action: lateral force at the pendulum base.
\end{itemize}

The corresponding controller architecture was devised as a two layer
neural field. Each layer of the field is actually a neural
population modeled after a subset of the real line: $x | x \in
(-1,1)$. 
Those layers have the following roles:

\begin{itemize}
\item A first layer, without inner dynamics (i.e. not governed by a
  differential equation) which merely maps the input variables to
  activation potentials on a field, using the vector coding technique.
\item A second layer, with prototypical dynamics of neural fields. The
  kernel function was used as the tuning/evolution parameter from
  which the control behavior emerged. The actual output was obtained
  by finding a weighted average of the activation potentials and
  mapping the resulting centroid to a single value (an inverse process
  to vector coding).
\end{itemize}

While this approach yields favorable results, there are some
properties of neural fields that are worth exploring. Of those, this chapter
focuses  on the way information is stored and processed on neural
fields, and how several strategies in this respect produce different
controllers.

In the following sections the initial approach of storing and
proccesing information will be modelled, detailed and modified, to get
3 different controller structures: Proportional-like controllers,
fuzzy-like controllers and wavelet-like controllers. Next, these
controller structures will be put to test against a simple biped
walking model problem, and compared against a classical proportional
state controller. Finally, some conclusions are made.

\section{Information Processing in the Neural Field
  Controller Architecture}
\label{sec:fuzzy-control}

Here is introduced a simple model of information processing in neural
field controllers to help the understanding of commonalities and
variations of the several controller structures developed next. It is
not meant as a global framework for information processing in neural
fields. It is instead, an specific model for the description of
controllers based on layered neural fields. The controller
architecture that will be used here onwards is described with an
enfasis in how information is processed in the controller.

The neural field controller architecture is now structured as
follows. It has three layers: one input layer, one processing layer
and one output layer. As a strongly layered architecture, the
connections are only allowed from contiguous layers, in one
direction. Thus, elements in one layer cannot receive inputs from
other elements except those in the immediately preceding layer (with
\emph{output} receiving from \emph{processing}, and \emph{processing}
receiving from \emph{input}. From them, only the processing layer is
composed of neural populations (with structure and dynamics). The
elements in the input layer have the structure, but not the behavior,
of neural populations, and include a mapping function from an input
variable to the activation potential of its population
structure. Finally, the elements in the output layer consist only each
of a mapping function from the activation potential of a processing
layer population to the value of an output variable. The layers are
further described below:

\paragraph{Input Layer}
Each population on this layer is used as a spacial representation of a
function of the inputs. The values stored in the population act only
as a buffer from which feed the inputs to the processing layer
populations, and correspond in each iteration to the mapping of an
input variable. While the mapping function can be general, here it
will be simplified. As a result, it is used one input layer population
for each relevant state of the controlled system (where relevant means
related to the control goal). It should be noted that this layer is
not ruled by a differential equation and therefore does not follow
Amari's definition of a neural field.

% The function used depends on the space where the state variable is
% defined. For an angular value $\theta_{in}$, it assigns an unitary
% potential to the neural population element located (or parameterized
% by) $\theta_{in}$.

For an input variable $e_i(t') \in \mathbb{R}$, the input mapping
function of input layer population $i$, assigns a potential
$u_i(\theta,t) \in \mathbb{R}$ to each position $theta$ in the input
layer population, at the current time $t$. The input variable
$e_i(t)$ is meant to represent the error value on a certain control
variable as a function of time.
\begin{equation}
  \label{eq:eqn-inpos}
  u_i(\theta,t)=f_i(e_i(t'))
\end{equation}
Note that the instantaneous value of $u_i(\theta,t)$ is a function of
the input variable $e_i(t')$ for $t' \in [0,t]$, and therefore the
mapping function has access to the entire history of $e_i$. In the
simplest case, the mapping function will depend only on the current
value $e_i(t)$.

\paragraph{Processing Layer}
Neural populations on this layer behave according to a differential
equation where the inputs are the functions of the potentials of
populations (for simplicity it will be assumed that only one
population in the preceding layer is an input to a given population,
and that input layer population is not shared among processing layer
populations). Therefore, those populations in this layer follow the
definition given by Amari for neural fields, but with an specific
structure for the interconnection between input layer and processing
layer populations.

The differential equation for the potential at position $\theta$ in a
population $j$ of the processing layer is:

\begin{align}
  \label{eq:lnf-oned}
  \tau_j\dot{u}_j(\theta,t)&=-u_j(\theta,t)
    +\int_{\Theta}{w_{j,j}(\theta,\theta') 
    \phi_j \left(u_j(\theta',t) \right) d\theta'} + S_j(\theta,t) \\
  S_j(\theta,t)&=\sum_{X \in
    \mathrm{In}}\int_{-\pi}^{\pi}{w_{X,Y}\left(s_Y(\theta,T_{X,Y}(\theta'))
    \right) f\left(u_X(\theta') \right) d\theta'}
\end{align}
Where as in previous chapters $\tau_Y$ is a time constant for the
population $Y$, $u_Y$ is the activation potential of elements in the
population, $w_{X,Y}$ is the connection kernel for connections from
population $X$ towards population $Y$, $s_Y$ is distance (or geodesic
length) between two positions on the underlying space of population
$Y$ using the given metric tensor for that space, $X \in $In is a
population in the Input Layer, and $T_{X,Y}$ is a transformation of
the orthogonal symmetry group of the circle O(2) fixed for the ordered
pair of populations.

The connection kernel $w_{X,Y}$ will once again take the form of a
Mexican-hat function, but given the underlying periodical nature of
the space, we perform a normalization of it (similarly to the
'homogenization' as proposed in
\cite{Ermentrout98Neural}). For a underlying compact space, there is a
diameter $D$ defined as the supremum of the distance between two
points in the space. The normalized connection kernel is thus: 

\begin{align}
\label{eq:norm-kernel}
  W(s)&=ke^{-(s)^2/(\delta \cdot D)^2}-H_0\\
  w(s)&=\frac{W(s)}{\int_0^1W(s)ds}
\end{align}

\paragraph{Output Layer}
% Transformation (symmetry group) and summation
This layer is used as a representation of the output values of the
controller architecture. As such, there will be one output population
for each output expected (or actuator controlled).

Similarly as it was done between the Input Layer and the Processing
Layer, the mapping between the Processing Layer and the Output Layer
is evaluated, for each output population, as the summation of the
transformation of each processing population. In other words, the
activation of an element located at $\theta$ in the output population
$Z$ is calculated as:

\begin{equation}
  \label{eq:eqn-outlayer}
  u_Z(\theta)=\sum_{Y \in \mathrm{Pr}}\alpha_{Y,Z}f(u_Y(T_{Y,Z}^{-1}(\theta))
\end{equation}
Where $\alpha_Y \in [0,1]$ is a coefficient for each processing
population, $Y \in$Pr is a population in the processing layer, and
$T_{Y,Z}^{-1}$ is the inverse of the mapping $T_{Y,Z} \in O(2)$.
% Centroid
The chosen output position $\Theta_Z \in [-\pi,\pi)$ for each output
population is then calculated as an the centroid of the above defined
population potential:

%TODO: alpha y la sumatoria pueden bajarse a esta eq

\begin{equation}
  \label{eq:eqn-outpos}
  \Theta_Z=\frac{\int_{-\pi}^{\pi}\theta \cdot u_Z(\theta)d\theta}{\int_{-\pi}^{\pi}u_Z(\theta)d\theta}
\end{equation}

% Stereographic Projection
Finally, $\Theta_Z$ is mapped to an output value $x_Z \in \mathbb{R}$
by means of the reverse stereographic projection given by:

\begin{equation}
  \label{eq:eqn-outval}
  x=\tan(\frac{\theta_Z}{2})
\end{equation}
%%%

% A simplistic overview of the way information is stored on the neural
% fields could be stated as follows: On each iteration of the evaluation
% algorithm, the state of the neural field is given by the activation
% potential at each spot on the field.  That means, there is a high
% number of parameters that grows with the detail of the discretization
% applied to the field on each dimension. With the aforementioned scheme,
% with all those parameters, essentially only a single value is stored,
% in such a way that resembles truth values in fuzzy logic. For each
% input layer, the value of a single scalar input variable is stored;
% and for each processing layer, the value of a single output control
% signal is stored (which is reduced to scalar form by an weighted
% average, as explained above).


\section{Three Neural Field Controller Structures}
\label{sec:control-struc}

\subsection{Proportional-like Controllers}
\label{sec:prop-control}


\subsection{Fuzzy-like Controllers}
\label{sec:fuzzy-control}


\subsection{Wavelet-like Controllers}
\label{sec:wavelet-control}






naive topological
structure of the populations, as they are modeled as close as possible
to the control action space, but present singularities at the extreme
points of the real line segment. Those singularities are not present
in the model developed by Amari, and can be expected to alter the
behavior of the population as the points drift away from the origin.

The second one is the parameterization scheme applied. The variation
of the kernel function, represented as a value list, preclude the
analysis of the inner workings of the control architecture. As the
resulting function gives no guaranties about its structure (not even
continuity or differentiability). Also they somehow water down the
spatial structure: there is no standard interpretation about the
relationship between close or distant elements in the population.

A third aspect is the scalability of the architecture. Having only an
input layer and an action layer, the control architecture has no room
for separation between the control goals and the control actions. A
first consequence of that is the lack of flexibility to have a scheme
with two control goals and two control actions (where each control
goal is supossed to act on each one of the control actions).

In what remains of this chapter, we present changes to the
control architecture and the topology of the population specifically
aimed to address these limitations. We also use the proposed
architecture to control the inverted pendulum with two goals:
stability and linear position.

\section{Topological Structures for Multiple Goals}
\label{sec:chp3-topological}
\subsection{Topology of Neural Populations}
As said previously, each neural population was previously modeled
after a segment of the real line. The obtained topology has two
extremal point not so well behaved. Particularly, those elements
laying at the extremal point will only have neighbors at one
side. Also, the number of neighboring elements located on each side
will be different, with exception to the middle element. Therefore,
the translational symmetry present in the model of Amary is lost, and
instead there is only a reflexive symmetry at the middle element.

It would be convenient to use a space that is closed and bounded, that
viewed as a manifold does not have boundary, and that has a continuous
symmetry group. This can be attained by applying the one-point
Alexandroff compactification of the Real line, by including a $\infty$
element. Therefore, the neural population will be modeled after a
unitary circle embedded in $\mathbb{R}^2$. It is defined as the
Co-domain of a map $f: (-\pi,\pi]\rightarrow \mathbb{R}^2$, $f:\theta
\mapsto r=(x_1,x_2)$ where:
\begin{align}
  x_1 &= cos(\theta)\\
  x_2 &= sin(\theta)
\end{align}

Consequentky, the emerging symmetry group is the orthogonal group
O(2), composed by the rotations of the circle SO(2) and its
reflections.

In order to evaluate the kernel function, it is needed a metric over
the above defined space. Given the Euclidean metric on $\mathbb{R}^2$,
the distance between two points located on the circle, parameterized
by $\theta_1$ y $\theta_2$, will be evaluated as the lower valued path
integral over the circle (the geodesic).
\begin{equation}
  \int_{\theta_1}^{\theta_2}\sqrt{\left(\frac{dx_1}{d\theta}\right)^2
    + \left(\frac{dx_2}{d\theta}\right)^2}d\theta
\end{equation}

Nonetheless, we also intend to modify the parameterization scheme used
previously, where the kernel function was used directly as the set of
parameters. Instead, we will provide each population with its own
metric. Consequently, using a positive definite symmetric matrix, a
metric tensor $g$, the actual expression used to evaluate the distance
is:
\begin{equation}
  s(\theta_1,\theta_2)=\min\left(\int_{\theta_1}^{\theta_2}\sqrt{\dot{r}(\theta)^TG\dot{r}(\theta)}d\theta,\int_{\theta_2}^{\theta_1}\sqrt{\dot{r}(\theta)^TG\dot{r}(\theta)}d\theta\right)
\end{equation}

One remaining issue to be solved is how to evaluate the distance
between two elements in different populations. As in general two
populations are not path connected, the above presented notion of
metric does not apply. That distance is needed when an element of a
population $a \in A$ receives as input the potential of an element $b
\in B$, or using another terminology, when $b$ is in the receptive
field of $a$. Therefore, to evaluate the distance, a simple rule can
be applied.

In general, the distances between an element $a$ and another element
$b$ will be evaluated using the metric of the receiving element (the
owner of the receptive field). In order to allow that, each element
$b$ in a population $B$ will be mapped to a location in the underlying
space $Y$ of that population. As every population is modeled after a
S1 space (differing only in the metric tensor), to map the element, it
will be used the O(2) symmetry group.

More explicitly, for an element with position in the underlying space
$y \in Y$, and another element with position in another underlying
space $x \in X$ in the receptive field of the element at $y$, the
distance $d(y,x)$ will be evaluated using a function $T_{X,Y}: X
\rightarrow Y$, $T_{X,Y}:\theta \mapsto T_{X,Y}(theta)$ where the
actual mapping will be given by applying an element of the circle
symmetry group O(2). The element of O(2) will be the same for the
entire mapping (in the simplest case, the identity element can be
applied, so that the element will keep its $\theta$ parameterization,
but will be assumed to lay in the target space). With this definition
we obtain a distance $d(x,y)=s(\theta_x,T_{Y,X}(\theta_y))$, that is
applicable even when $x,y \in Y$, where it will be used the identity
transformation $T_{Y,Y}=1_Y$. It is worth noting, that in the general
case $d(x,y)$ cannot be considered as a complete metric over the union
of several spaces because it is not symmetric (i.e. $d(x,y) \neq
d(y,x)$), because $d(y,x)$ may even be not defined (when $x$ is not in
the receptive field of $y$).


\subsection{Neural Field Controller Architecture}
In this chapter, the neural field controller architecture that will be
used has three layers: one input layer, one processing layer and one
output layer. Each layer has several neural populations (each one with
its own underlying space and metric). A neural population in one layer
cannot receive inputs from other populations except those in the
immediately preceding layer (with \emph{output} receiving from
\emph{processing}, and \emph{processing} receiving from
\emph{input}. The layer are further described below:

\paragraph{Input Layer}
Each population on this layer is used as a spacial representation of a
function of the inputs. While this function can be general, here it
will be simplified. As a result, it is used one input population for
each relevant state of the controlled system (where relevant means
related to the control goal). It should be noted that this layer is
not ruled by a differential equation (also do not use a metric or
kernel) and therefore are not related with Amari's definition of a
neural field.

% The function used depends on the space where the state variable is
% defined. For an angular value $\theta_{in}$, it assigns an unitary
% potential to the neural population element located (or parameterized
% by) $\theta_{in}$.

For an input value $x_{in} \in \mathbb{R}$, the input mapping function
assign a unitary potential to the element located in position obtained
by the reverse stereographic projection (as shown in the figure):
%TODO: include figure
\begin{equation}
  \label{eq:eqn-inpos}
  \theta=2\arctan \left(x_{in} \right)
\end{equation}

\paragraph{Processing Layer}
Neural populations on this layer behave according to a differential
equation where the inputs are the potentials of other elements of the
population, as well of those elements in other populations of the
previous layer that are in the receptive field of the element (for
simplicity it will be assumed that every population in the preceding
layer is in the receptive field of each element in a given
population. Therefore, those populations in this layer are parallel to
the definition given by Amari for neural fields (they differ in the
use of metric tensors, in the topology of the underlying space and in
the mapping used to connect elements in one population to elements in
another one).

The differential equation for the potential of an element at $\theta$
in a population $Y$ of the processing layer is:

\begin{align}
  \label{eq:lnf-oned}
  \tau_Y\dot{u}_Y(\theta)&=-u_Y(\theta)+\int_{-\pi}^{\pi}{w_{Y,Y}\left(s_Y(\theta,\theta')
    \right) f\left(u_Y(\theta') \right) d\theta'} + S_Y(\theta) \\
  S_Y(\theta)&=\sum_{X \in
    \mathrm{In}}\int_{-\pi}^{\pi}{w_{X,Y}\left(s_Y(\theta,T_{X,Y}(\theta'))
    \right) f\left(u_X(\theta') \right) d\theta'}
\end{align}
Where as in previous chapters $\tau_Y$ is a time constant for the
population $Y$, $u_Y$ is the activation potential of elements in the
population, $w_{X,Y}$ is the connection kernel for connections from
population $X$ towards population $Y$, $s_Y$ is distance (or geodesic
length) between two positions on the underlying space of population
$Y$ using the given metric tensor for that space, $X \in $In is a
population in the Input Layer, and $T_{X,Y}$ is a transformation of
the orthogonal symmetry group of the circle O(2) fixed for the ordered
pair of populations.

The connection kernel $w_{X,Y}$ will once again take the form of a
Mexican-hat function, but given the underlying periodical nature of
the space, we perform a normalization of it (similarly to the
'homogenization' as proposed in
\cite{Ermentrout98Neural}). For a underlying compact space, there is a
diameter $D$ defined as the supremum of the distance between two
points in the space. The normalized connection kernel is thus: 

\begin{align}
\label{eq:norm-kernel}
  W(s)&=ke^{-(s)^2/(\delta \cdot D)^2}-H_0\\
  w(s)&=\frac{W(s)}{\int_0^1W(s)ds}
\end{align}

\paragraph{Output Layer}
% Transformation (symmetry group) and summation
This layer is used as a representation of the output values of the
controller architecture. As such, there will be one output population
for each output expected (or actuator controlled).

Similarly as it was done between the Input Layer and the Processing
Layer, the mapping between the Processing Layer and the Output Layer
is evaluated, for each output population, as the summation of the
transformation of each processing population. In other words, the
activation of an element located at $\theta$ in the output population
$Z$ is calculated as:

\begin{equation}
  \label{eq:eqn-outlayer}
  u_Z(\theta)=\sum_{Y \in \mathrm{Pr}}\alpha_{Y,Z}f(u_Y(T_{Y,Z}^{-1}(\theta))
\end{equation}
Where $\alpha_Y \in [0,1]$ is a coefficient for each processing
population, $Y \in$Pr is a population in the processing layer, and
$T_{Y,Z}^{-1}$ is the inverse of the mapping $T_{Y,Z} \in O(2)$.
% Centroid
The chosen output position $\Theta_Z \in [-\pi,\pi)$ for each output
population is then calculated as an the centroid of the above defined
population potential:

%TODO: alpha y la sumatoria pueden bajarse a esta eq

\begin{equation}
  \label{eq:eqn-outpos}
  \Theta_Z=\frac{\int_{-\pi}^{\pi}\theta \cdot u_Z(\theta)d\theta}{\int_{-\pi}^{\pi}u_Z(\theta)d\theta}
\end{equation}

% Stereographic Projection
Finally, $\Theta_Z$ is mapped to an output value $x_Z \in \mathbb{R}$
by means of the reverse stereographic projection given by:

\begin{equation}
  \label{eq:eqn-outval}
  x=\tan(\frac{\theta_Z}{2})
\end{equation}

\section{Evolution of the Controller Architecture}
\label{sec:chp3-evolution}
\subsection{Acquisition of Objectives by Evolution}
For the evolution process, with will be used the HAEA algorithm as
proposed in \cite{JonantanHaea} and its implementation in the library
JML. For each control goal it will be applied an adaptation by means
of the evolutionary algorithm as follows:

\begin{itemize}
\item The first adaptation will be performed taking into account only
  the first goal, and there will be evolved the input populations, the
  only processing population, and the output populations.

\item Following adaptations will be performed considering the next
  objective, so that the new parameterization coordinates its behavior
  with preceding ones. Therefore, in each new adaptation, the existing
  parameterizations of the input layer and the existing processing
  populations will remain unaltered. A processing population will be
  added and parameterized, and also the output layer will be
  re-parameterized (and output populations added explicitly if
  needed).
\end{itemize}

\subsection{Genotypic Representation and Evolution Operators}

In-Pr: $\alpha_{X,Y}$ %$T_{X,Y}$ (boolean, double)
Pr: $L_y$ (double, double) %$\delta_y$, $k_y$, $H_y$
Pr-Out: $T_{Y,Z}$ (boolean, double), $\alpha_{Y,Z}$

CARD(X)=4
CARD(Y)=2
CARD(Z)=1

CARD(X*Y)=8
CARD(Y*Z)=2

8
4
2+4	

TOTAL PARAMS= 32 (10 boolean, 22 double)
% \section{Co-evolution and Complexification}
% \label{sec:chp3-coevolution}

\section{Experimental Framework}
\label{sec:chp3-experimental}

\section{Experimental Results}
\label{sec:chp3-results}

\section{Discussion}
\label{sec:chp3-discussion}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 

